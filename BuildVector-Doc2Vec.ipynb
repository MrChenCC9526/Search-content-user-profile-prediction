{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import jieba\n",
    "import config # 自定义配置文件\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "# 导入自定义模块\n",
    "sys.path.append(config.Py_path) # 添加路径\n",
    "from SaveAndLoad import save_pkl # 数据文件持久化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple # 创建命名元组，即创建和tuple类似的对象\n",
    "import subprocess # 子进程\n",
    "import codecs # 编码转换\n",
    "from gensim.models import Doc2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 构建Doc2Vec模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义一个namedtuple类型SentimentDocument（对象），并包含SentimentDocument和words tags属性\n",
    "SentimentDocument = namedtuple('SentimentDocument', 'words tags')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义文档创建类\n",
    "class Doc_list(object):\n",
    "    def __init__(self,f):\n",
    "        self.f = f\n",
    "    def __iter__(self):\n",
    "        Participle_Jieba = pd.read_excel(self.f)\n",
    "        for i,line in enumerate(Participle_Jieba[\"token\"]):\n",
    "            words = line[2:-2].split(\"', '\") # 划分分词结果\n",
    "            tags = [i]\n",
    "            yield SentimentDocument(words,tags) # yield——生成器，减少占用的内存"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 模型实例化\n",
    "D2V = Doc2Vec(dm=0, vector_size=300, negative=5, hs=0, min_count=3, window=30,sample=1e-5,workers=8,alpha=0.025,min_alpha=0.025)\n",
    "D2V_HW = Doc2Vec(dm=0, vector_size=300, negative=2, hs=0, min_count=1, window=5,sample=1e-5,workers=8,alpha=0.025,min_alpha=0.025)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "doc_list = Doc_list(config.WordData_path + 'Participle-Jieba.xlsx') # 生成文档\n",
    "HW_doc_list = Doc_list(config.WordData_path + \"Participle-FoolNLTK-HW.xlsx\") # 生成文档"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "D2V.build_vocab(doc_list) # 构建词汇表\n",
    "D2V_HW.build_vocab(HW_doc_list) # 构建词汇表"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "D2V.train(doc_list,total_examples=17651, epochs=5) # Doc2Vec模型训练\n",
    "D2V_HW.train(HW_doc_list,total_examples=D2V_HW.corpus_count, epochs=5) # Doc2Vec模型训练"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 词向量持久化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 词向量提取\n",
    "X_sp = np.array([D2V.docvecs[i] for i in range(17651)])\n",
    "X_HW_sp = np.array([D2V_HW.docvecs[i] for i in range(17651)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(17651, 300)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_sp.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(17651, 300)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_HW_sp.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "持久化存储路径：./data/WordVectorData/D2V_X_sp.feat\n"
     ]
    }
   ],
   "source": [
    "# 存储词向量数据\n",
    "save_pkl(X_sp,\"D2V_X_sp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "持久化存储路径：./data/WordVectorData/D2V_HW_X_sp.feat\n"
     ]
    }
   ],
   "source": [
    "# 存储词向量数据\n",
    "save_pkl(X_HW_sp,\"D2V_HW_X_sp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

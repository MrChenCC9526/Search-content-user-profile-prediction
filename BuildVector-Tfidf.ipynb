{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import jieba\n",
    "import config # 自定义配置文件\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# 不显示VisibleDeprecation警告\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=Warning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 导入数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(config.AF_Data_path + \"AFAfter.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 17651 entries, 0 to 17650\n",
      "Data columns (total 16 columns):\n",
      "Education     17651 non-null int64\n",
      "Id            17651 non-null object\n",
      "age           17651 non-null int64\n",
      "gender        17651 non-null int64\n",
      "query         17651 non-null object\n",
      "SpaceNum      17651 non-null float64\n",
      "SpaceRATIO    17651 non-null float64\n",
      "LinkNum       17651 non-null float64\n",
      "LinkRATIO     17651 non-null float64\n",
      "TextSum       17651 non-null float64\n",
      "TextMax       17651 non-null float64\n",
      "TextMin       17651 non-null float64\n",
      "TextMedian    17651 non-null float64\n",
      "TextMean      17651 non-null float64\n",
      "SearchNum     17651 non-null float64\n",
      "HighWords     17651 non-null object\n",
      "dtypes: float64(10), int64(3), object(3)\n",
      "memory usage: 2.2+ MB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 导入自定义词库"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建停用词列表\n",
    "def stopwordslist():\n",
    "    # 获取停用词文件列表\n",
    "    for root, dirs, files in os.walk(config.stopword_path):\n",
    "        pass\n",
    "    # 停用词list\n",
    "    stopwords = []\n",
    "    for filename in files:\n",
    "        for line in open(config.stopword_path + filename, 'r+', encoding='utf-8').readlines():\n",
    "            stopword = line.strip()\n",
    "            stopwords.append(stopword)\n",
    "    return list(set(stopwords))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建新增词汇\n",
    "def add_Words():\n",
    "    # 获取自定义词典文件名\n",
    "    for root, dirs, files in os.walk(config.addword_path):\n",
    "        filenames = [filename for filename in files if 'txt' in filename]\n",
    "    addWords = []\n",
    "    for filename in filenames:\n",
    "        for line in open(config.addword_path + filename, 'r+').readlines():\n",
    "            addword = line.strip()\n",
    "            addWords.append(addword)\n",
    "    return list(set(addWords))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "addwordslist = add_Words() # 自定义词典\n",
    "stopwordslist = stopwordslist() # 停用词词典"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache C:\\Users\\11147\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 0.657 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    }
   ],
   "source": [
    "# jieba.add_word(word) # 新增词汇\n",
    "jieba.load_userdict(addwordslist) # 添加用户自定义词典"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 生成词向量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer # tfidf\n",
    "# 模型、数据文件保存\n",
    "from sklearn.externals import joblib\n",
    "from sklearn.datasets import load_boston\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 停用词处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Deactivate_Words(words,stopwordslist):\n",
    "    if len(words) <= 2:\n",
    "        word_After_Stop = words\n",
    "    elif len(words) > 2:\n",
    "        word_After_Stop = list(set(words).difference(stopwordslist)) \n",
    "    return word_After_Stop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 分词处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tokens(wordtxt):\n",
    "    tokens = [] # 用于存储分词结果\n",
    "    for query in wordtxt.split('\\t'): # 使用 \\t 识别搜内容文本\n",
    "        words = [word for word in jieba.cut(query)] # 分词\n",
    "        words = Deactivate_Words(words,stopwordslist) # 停用词处理\n",
    "#         print(words)\n",
    "        # 分词结果进行拼接；强化词语语义\n",
    "        for gram in [1,2]:\n",
    "            for i in range(len(words) - gram + 1): # 根据分词结果列表进行拼接，最大拼接长度：2\n",
    "                tokens += [\"_*_\".join(words[i:i+gram])] # 使用join拼接词语，并将结果添加至tokens\n",
    "#                 print(\"_*_\".join(words[i:i+gram]))\n",
    "#         print(\"-\"*20)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 生成搜索内容词向量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 重写字符串标记化\n",
    "class Tokenizer():\n",
    "    def __init__(self):\n",
    "        self.n = 0\n",
    "    def __call__(self,line): # 可调用对象\n",
    "        tokens = get_tokens(line)\n",
    "        if np.random.rand() < 0.00001: # 随机输出一个切分样例\n",
    "            print(line)\n",
    "            print('='*20)\n",
    "            print(tokens)\n",
    "        self.n += 1\n",
    "        if self.n%1000==0:\n",
    "            print(\"已重写字符串记录数：\" + str(self.n),end=' ')\n",
    "        return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 原始文档集合转换为TF-IDF特征矩阵\n",
    "TF_IDF = TfidfVectorizer(tokenizer=Tokenizer(), # 指定分割的方式,用于将字符串分割为一系列标记函数\n",
    "                      min_df=3, # \n",
    "                      max_df=0.95, # \n",
    "                      sublinear_tf=True # \n",
    "                     ) # 实例化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "已重写字符串记录数：1000 已重写字符串记录数：2000 已重写字符串记录数：3000 已重写字符串记录数：4000 已重写字符串记录数：5000 已重写字符串记录数：6000 九洲天空城结局\t一世忘川\t陈百强\t重生不负\t得非所愿by\t揣着包子带球跑\t得非所愿by忍冬\t重生之总裁来爱我\timoo手机\t星际第一海盗王\t悄悄告诉他黑白剑妖\t末世重生之累赘\t极速前进第三季\t买卖不成仁义在夏日\tjackson周嘉诚\t宋佳\t蒲姓取名\t我只喜欢你\t君颜再归\t光影边缘\t我去上学啦2\t腐书网2016\t重生之太上皇嫁到\t忠犬炼成手册\t苹果7\t买卖不成仁义在\t香菇炖鸡\t我不是死了吗白花花\t杨幂\t许韵珊\t外星老公你走开\t恶魔之名\t戴向宇\t吃什么补抗体\t俏王爷强占山大王\t色素之死性不改\t王俊凯\t章鱼不哭\t蒲姓男孩最好的取名\t重生之不可能的替身\t论穿越到未来的风险性\t周嘉诚\t重生之有你相伴\t重生之嫡子逆袭\t禹成贤\t从心开始\t香菇\t渣攻,不再见\tukiss kevin\t赵文卓\t周嘉诚天天向上\t总有食材想吃掉我\t极速前进\t有匪美人来袭\t一生相守\t哪只总裁的替身\t遇见你,要定你\t放开我北鼻\t重生之护夫狂魔\t你的我的爱的礼物\t炫浪网络社区\t陈泰鸣\t重生之我本君子\t张国荣\t你没事儿老瞅我干哈呀\t相思不匪\t重生之一盟情深\t鸿星尔克官网\t动漫电影\t罗嘉良\t七年之痒\t安踏官方旗舰店\t唐生\t愿非所得\t蝴蝶谷\t2016综艺节目单\tjackson\t放开我北鼻jackson\t重生之影帝家的古代萌妻\t第一个故事谁爱他的妻子\t仵作先生(穿越 悬疑推理 7)\t冷总裁的贴身妖孽\t无法泅渡\t我不是死了吗\t邓丽君\t安全范围\t重生之深藏不露\t唐鹤德\t梅艳芳\t末世重生之丧尸大哥求放过\t重生之陵渊求墨\t白敬亭\t腐书网\t上司和我的秘密\n",
      "\n",
      "====================\n",
      "['九洲', '天空', '结局', '城', '九洲_*_天空', '天空_*_结局', '结局_*_城', '一世', '忘川', '一世_*_忘川', '陈百强', '重生', '不负', '重生_*_不负', '非所愿', '揣着', '带球', '包子', '跑', '揣着_*_带球', '带球_*_包子', '包子_*_跑', '忍冬', '非所愿', '忍冬_*_非所愿', '总裁', '重生', '爱', '总裁_*_重生', '重生_*_爱', 'imoo', '手机', 'imoo_*_手机', '第一', '星际', '海盗王', '第一_*_星际', '星际_*_海盗王', '告诉', '妖', '黑白', '悄悄', '剑', '告诉_*_妖', '妖_*_黑白', '黑白_*_悄悄', '悄悄_*_剑', '末世', '重生', '累赘', '末世_*_重生', '重生_*_累赘', '第三季', '极速', '第三季_*_极速', '仁义', '买卖', '夏日', '仁义_*_买卖', '买卖_*_夏日', 'jackson', '周嘉诚', 'jackson_*_周嘉诚', '宋佳', '蒲姓', '取名', '蒲姓_*_取名', '喜欢', '君颜', '光影', '边缘', '光影_*_边缘', '上学', '腐书网', '2016', '腐书网_*_2016', '嫁', '重生', '太上皇', '嫁_*_重生', '重生_*_太上皇', '手册', '炼成', '忠犬', '手册_*_炼成', '炼成_*_忠犬', '苹果', '7', '苹果_*_7', '仁义', '买卖', '仁义_*_买卖', '香菇', '炖鸡', '香菇_*_炖鸡', '死', '白花花', '死_*_白花花', '杨幂', '许韵', '珊', '许韵_*_珊', '老公', '走开', '外星', '老公_*_走开', '走开_*_外星', '恶魔', '之名', '恶魔_*_之名', '戴向宇', '抗体', '补', '吃', '抗体_*_补', '补_*_吃', '强占', '俏', '王爷', '山大王', '强占_*_俏', '俏_*_王爷', '王爷_*_山大王', '死性不改', '色素', '死性不改_*_色素', '王俊凯', '章鱼', '不哭', '章鱼_*_不哭', '蒲姓', '男孩', '取名', '蒲姓_*_男孩', '男孩_*_取名', '替身', '重生', '替身_*_重生', '风险性', '穿越', '未来', '风险性_*_穿越', '穿越_*_未来', '周嘉诚', '之有', '重生', '相伴', '之有_*_重生', '重生_*_相伴', '嫡子', '重生', '逆袭', '嫡子_*_重生', '重生_*_逆袭', '禹成贤', '从心', '开始', '从心_*_开始', '香菇', '见', '渣攻', '见_*_渣攻', ' ', 'kevin', 'ukiss', ' _*_kevin', 'kevin_*_ukiss', '赵文卓', '周嘉诚', '天天向上', '周嘉诚_*_天天向上', '吃掉', '食材', '想', '总有', '吃掉_*_食材', '食材_*_想', '想_*_总有', '极速', '前进', '极速_*_前进', '美人来', '有匪', '袭', '美人来_*_有匪', '有匪_*_袭', '一生', '相守', '一生_*_相守', '总裁', '替身', '总裁_*_替身', '遇见', '要定', '遇见_*_要定', '我北', '鼻', '放开', '我北_*_鼻', '鼻_*_放开', '之护夫', '魔', '重生', '狂', '之护夫_*_魔', '魔_*_重生', '重生_*_狂', '爱', '礼物', '爱_*_礼物', '炫浪', '社区', '网络', '炫浪_*_社区', '社区_*_网络', '陈泰鸣', '君子', '重生', '君子_*_重生', '张国荣', '老', '没事儿', '干哈', '瞅', '老_*_没事儿', '没事儿_*_干哈', '干哈_*_瞅', '相思', '不匪', '相思_*_不匪', '情深', '重生', '盟', '情深_*_重生', '重生_*_盟', '鸿星', '尔克', '官网', '鸿星_*_尔克', '尔克_*_官网', '动漫', '电影', '动漫_*_电影', '罗嘉良', '七年之痒', '安踏', '官方', '旗舰店', '安踏_*_官方', '官方_*_旗舰店', '唐生', '愿非', '所得', '愿非_*_所得', '蝴蝶谷', '综艺节目', '2016', '单', '综艺节目_*_2016', '2016_*_单', 'jackson', '我北', '鼻', '放开', 'jackson', '我北_*_鼻', '鼻_*_放开', '放开_*_jackson', '影帝', '萌妻', '家', '古代', '重生', '影帝_*_萌妻', '萌妻_*_家', '家_*_古代', '古代_*_重生', '妻子', '第一个', '爱', '故事', '妻子_*_第一个', '第一个_*_爱', '爱_*_故事', ' ', '仵作', '悬疑', '穿越', '推理', ' _*_仵作', '仵作_*_悬疑', '悬疑_*_穿越', '穿越_*_推理', '妖孽', '总裁', '冷', '贴身', '妖孽_*_总裁', '总裁_*_冷', '冷_*_贴身', '无法', '泅渡', '无法_*_泅渡', '死', '邓丽君', '安全', '范围', '安全_*_范围', '重生', '深藏不露', '重生_*_深藏不露', '唐鹤德', '梅艳芳', '求', '丧尸', '大哥', '末世', '重生', '放过', '求_*_丧尸', '丧尸_*_大哥', '大哥_*_末世', '末世_*_重生', '重生_*_放过', '墨', '重生', '之陵渊求', '墨_*_重生', '重生_*_之陵渊求', '白敬亭', '腐书网', '\\r\\n', '上司', '秘密', '\\r\\n_*_上司', '上司_*_秘密']\n",
      "已重写字符串记录数：7000 已重写字符串记录数：8000 已重写字符串记录数：9000 已重写字符串记录数：10000 已重写字符串记录数：11000 已重写字符串记录数：12000 已重写字符串记录数：13000 已重写字符串记录数：14000 已重写字符串记录数：15000 已重写字符串记录数：16000 已重写字符串记录数：17000 "
     ]
    }
   ],
   "source": [
    "X_sp = TF_IDF.fit_transform(data['query'].values.astype('U')) # 拟合模型并返回文本矩阵"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 生成高频词词向量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 原始文档集合转换为TF-IDF特征矩阵\n",
    "TF_IDF_HW = TfidfVectorizer(min_df=3,max_df=0.95,sublinear_tf=True) # 实例化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "HW_sp = TF_IDF_HW.fit_transform(data[\"HighWords\"]) # 拟合模型并返回文本矩阵"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 持久化"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 模型文件持久化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\Anaconda3_5.2.0\\lib\\site-packages\\sklearn\\externals\\joblib\\numpy_pickle.py:93: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
      "  pickler.file_handle.write(chunk.tostring('C'))\n",
      "E:\\Anaconda3_5.2.0\\lib\\site-packages\\sklearn\\externals\\joblib\\numpy_pickle.py:93: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
      "  pickler.file_handle.write(chunk.tostring('C'))\n",
      "E:\\Anaconda3_5.2.0\\lib\\site-packages\\sklearn\\externals\\joblib\\numpy_pickle.py:93: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
      "  pickler.file_handle.write(chunk.tostring('C'))\n",
      "E:\\Anaconda3_5.2.0\\lib\\site-packages\\sklearn\\externals\\joblib\\numpy_pickle.py:93: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
      "  pickler.file_handle.write(chunk.tostring('C'))\n",
      "E:\\Anaconda3_5.2.0\\lib\\site-packages\\sklearn\\externals\\joblib\\numpy_pickle.py:93: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
      "  pickler.file_handle.write(chunk.tostring('C'))\n",
      "E:\\Anaconda3_5.2.0\\lib\\site-packages\\sklearn\\externals\\joblib\\numpy_pickle.py:93: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
      "  pickler.file_handle.write(chunk.tostring('C'))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['./Model/TF_IDF_HW.pkl']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(TF_IDF,config.Model_path + 'TF_IDF.pkl')\n",
    "joblib.dump(TF_IDF_HW,config.Model_path + 'TF_IDF_HW.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#载入模型\n",
    "TF_IDF_Model = joblib.load(config.Model_path + 'TF_IDF.pkl')\n",
    "TF_IDF_HW_Model = joblib.load(config.Model_path + 'TF_IDF_HW.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 数据文件持久化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 保存数据文件\n",
    "pkl_file = open(config.WV_Data_path + 'TFIDF_sp.feat','wb')\n",
    "pickle.dump(X_sp,pkl_file)\n",
    "pkl_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 保存数据文件\n",
    "pkl_file = open(config.WV_Data_path + 'TFIDF_HW_sp.feat','wb')\n",
    "pickle.dump(HW_sp,pkl_file)\n",
    "pkl_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载文件\n",
    "pkl_file = open(config.WV_Data_path + 'TFIDF_sp.feat','rb')\n",
    "T_X_sp= pickle.load(pkl_file)\n",
    "pkl_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载文件\n",
    "pkl_file = open(config.WV_Data_path + 'HW_sp.feat','rb')\n",
    "T_HW_sp= pickle.load(pkl_file)\n",
    "pkl_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 向量化数据文件持久化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "TFIDF_sp_toarray = TFIDF_sp.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "TFIDF_HW_sp_toarray = TFIDF_HW_sp.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "with h5py.File(config.WV_Data_path + \"TFIDF_sp_toarray.hdf5\", 'w') as f:\n",
    "    f.create_dataset(\"TFIDF_sp_toarray\", # TFIDF_sp_toarray:是HDF5文件中c数据储存的名字\n",
    "                     data=TFIDF_sp_toarray,  # 高维数组\n",
    "                     compression=\"gzip\", # compression=\"gzip\"：是对数据进行压缩，压缩的方式一个gzip\n",
    "                     compression_opts=5 # compression_opts=5：是压缩等级，压缩等级越高，压缩的越好，但是压缩消耗的CPU时间会增加\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 保存数据文件\n",
    "pkl_file = open(config.WV_Data_path + 'TFIDF_sp_toarray.feat','wb')\n",
    "pickle.dump(TFIDF_sp_toarray,pkl_file)\n",
    "pkl_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载文件\n",
    "with h5py.File(config.WV_Data_path + \"TFIDF_sp_toarray.hdf5\", 'r') as f:  # 读取的时候是‘r’\n",
    "    print(f.keys())\n",
    "    TFIDF_sp_toarray = f.get(\"TFIDF_sp_toarray\")[:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载文件\n",
    "pkl_file = open(config.WV_Data_path + 'TFIDF_HW_sp_toarray.feat','rb')\n",
    "TFIDF_HW_sp_toarray= pickle.load(pkl_file)\n",
    "pkl_file.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

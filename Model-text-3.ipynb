{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sys\n",
    "import os\n",
    "import pickle\n",
    "import config # 自定义配置文件\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense, Dropout, Activation\n",
    "from tensorflow.python.keras.utils import np_utils\n",
    "import xgboost as xgb\n",
    "from sklearn import svm\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 传入特征与目标列进行划分训练集与测试集\n",
    "def get_train_test(X,Y):\n",
    "    X_train, X_test,  Y_train, Y_test = train_test_split(X, # 特征\n",
    "                                                         Y, # 目标\n",
    "                                                         test_size = 0.3, # 测试集大小为30%\n",
    "                                                         random_state = 10)\n",
    "    return X_train, X_test,  Y_train, Y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(config.AF_Data_path + \"AFAfter.csv\")\n",
    "col_list = [\"Education\",\"age\",\"gender\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "W2V_nn_pred_df = pd.read_csv(config.FEData_path + \"W2V_nn.csv\")\n",
    "W2V_HW_pred_df = pd.read_csv(config.FEData_path + \"W2V_HW_nn.csv\")\n",
    "TFIDF_HW_pred_df = pd.read_csv(config.FEData_path + \"TFIDF_HW_nn.csv\")\n",
    "TFIDF_nn_pred_df = pd.read_csv(config.FEData_path + \"TFIDF_nn.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_ALL = np.array(pd.concat([W2V_nn_pred_df,W2V_HW_pred_df,TFIDF_HW_pred_df,TFIDF_nn_pred_df,\n",
    "                              data[['SpaceNum', 'SpaceRATIO','LinkNum', 'LinkRATIO', 'TextSum', \n",
    "                                    'TextMax', 'TextMin', 'TextMedian','TextMean', 'SearchNum',]]],axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tr, X_te, Y_tr, Y_te = get_train_test(pred_ALL,data[\"Education\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 目标列类别顺序化\n",
    "uq = Y_tr.unique() # 取目标列类别\n",
    "uq.sort() # 排序\n",
    "\n",
    "Y_tr[Y_tr == uq[-1]] = 0\n",
    "Y_te[Y_te == uq[-1]] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def XGB_Model(X,Y):\n",
    "    # 划分数据集\n",
    "    X_tr, X_te, Y_tr, Y_te = get_train_test(X,Y)\n",
    "    \n",
    "    # 目标列类别顺序化\n",
    "    uq = Y_tr.unique() # 取目标列类别\n",
    "    uq.sort() # 排序\n",
    "        \n",
    "    Y_tr[Y_tr == uq[-1]] = 0\n",
    "    Y_te[Y_te == uq[-1]] = 0\n",
    "    \n",
    "    # 模型实例化\n",
    "    XGB_Model = xgb.XGBClassifier(use_label_encoder=False)\n",
    "    # 模型训练\n",
    "    XGB_Model = XGB_Model.fit(X_tr,Y_tr)\n",
    "    print(XGB_Model)\n",
    "    # 模型预测\n",
    "    pred_train = XGB_Model.predict(X_tr)\n",
    "    pred_test = XGB_Model.predict(X_te)\n",
    "    \n",
    "    # 输出预测结果\n",
    "    train_acc = accuracy_score(Y_tr, pred_train)\n",
    "    test_acc = accuracy_score(Y_te, pred_test)\n",
    "    print (\"训练集准确率: {0:.4f}, 测试集准确率: {1:.4f}\".format(train_acc, test_acc))\n",
    "    \n",
    "    # 输出模型\n",
    "    return XGB_Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### W2V_XGB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### W2V_nn_XGB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = np.array(pd.concat([W2V_nn_pred_df,\n",
    "                          data[['SpaceNum', 'SpaceRATIO','LinkNum', 'LinkRATIO', 'TextSum', \n",
    "                                'TextMax', 'TextMin', 'TextMedian','TextMean', 'SearchNum',]]],axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[17:13:29] WARNING: D:\\Build\\xgboost\\xgboost-1.3.1.git\\src\\learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "              colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,\n",
      "              importance_type='gain', interaction_constraints='',\n",
      "              learning_rate=0.300000012, max_delta_step=0, max_depth=6,\n",
      "              min_child_weight=1, missing=nan, monotone_constraints='()',\n",
      "              n_estimators=100, n_jobs=12, num_parallel_tree=1,\n",
      "              objective='multi:softprob', random_state=0, reg_alpha=0,\n",
      "              reg_lambda=1, scale_pos_weight=None, subsample=1,\n",
      "              tree_method='exact', use_label_encoder=False,\n",
      "              validate_parameters=1, verbosity=None)\n",
      "训练集准确率: 0.8836, 测试集准确率: 0.4031\n"
     ]
    }
   ],
   "source": [
    "W2V_nn_XGB = XGB_Model(pred,data[\"Education\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### W2V_HW_XGB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = np.array(pd.concat([W2V_HW_pred_df,\n",
    "                          data[['SpaceNum', 'SpaceRATIO','LinkNum', 'LinkRATIO', 'TextSum', \n",
    "                                'TextMax', 'TextMin', 'TextMedian','TextMean', 'SearchNum',]]],axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[17:14:01] WARNING: D:\\Build\\xgboost\\xgboost-1.3.1.git\\src\\learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "              colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,\n",
      "              importance_type='gain', interaction_constraints='',\n",
      "              learning_rate=0.300000012, max_delta_step=0, max_depth=6,\n",
      "              min_child_weight=1, missing=nan, monotone_constraints='()',\n",
      "              n_estimators=100, n_jobs=12, num_parallel_tree=1,\n",
      "              objective='multi:softprob', random_state=0, reg_alpha=0,\n",
      "              reg_lambda=1, scale_pos_weight=None, subsample=1,\n",
      "              tree_method='exact', use_label_encoder=False,\n",
      "              validate_parameters=1, verbosity=None)\n",
      "训练集准确率: 0.8853, 测试集准确率: 0.4045\n"
     ]
    }
   ],
   "source": [
    "W2V_HW_XGB = XGB_Model(pred,data[\"Education\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TFIDF_XGB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TFIDF_nn_XGB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = np.array(pd.concat([TFIDF_nn_pred_df,\n",
    "                          data[['SpaceNum', 'SpaceRATIO','LinkNum', 'LinkRATIO', 'TextSum', \n",
    "                                'TextMax', 'TextMin', 'TextMedian','TextMean', 'SearchNum',]]],axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[17:14:24] WARNING: D:\\Build\\xgboost\\xgboost-1.3.1.git\\src\\learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "              colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,\n",
      "              importance_type='gain', interaction_constraints='',\n",
      "              learning_rate=0.300000012, max_delta_step=0, max_depth=6,\n",
      "              min_child_weight=1, missing=nan, monotone_constraints='()',\n",
      "              n_estimators=100, n_jobs=12, num_parallel_tree=1,\n",
      "              objective='multi:softprob', random_state=0, reg_alpha=0,\n",
      "              reg_lambda=1, scale_pos_weight=None, subsample=1,\n",
      "              tree_method='exact', use_label_encoder=False,\n",
      "              validate_parameters=1, verbosity=None)\n",
      "训练集准确率: 0.8687, 测试集准确率: 0.4052\n"
     ]
    }
   ],
   "source": [
    "TFIDF_nn_XGB = XGB_Model(pred,data[\"Education\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TFIDF_HW_XGB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = np.array(pd.concat([TFIDF_HW_pred_df,\n",
    "                          data[['SpaceNum', 'SpaceRATIO','LinkNum', 'LinkRATIO', 'TextSum', \n",
    "                                'TextMax', 'TextMin', 'TextMedian','TextMean', 'SearchNum',]]],axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[17:14:40] WARNING: D:\\Build\\xgboost\\xgboost-1.3.1.git\\src\\learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "              colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,\n",
      "              importance_type='gain', interaction_constraints='',\n",
      "              learning_rate=0.300000012, max_delta_step=0, max_depth=6,\n",
      "              min_child_weight=1, missing=nan, monotone_constraints='()',\n",
      "              n_estimators=100, n_jobs=12, num_parallel_tree=1,\n",
      "              objective='multi:softprob', random_state=0, reg_alpha=0,\n",
      "              reg_lambda=1, scale_pos_weight=None, subsample=1,\n",
      "              tree_method='exact', use_label_encoder=False,\n",
      "              validate_parameters=1, verbosity=None)\n",
      "训练集准确率: 0.8902, 测试集准确率: 0.4092\n"
     ]
    }
   ],
   "source": [
    "TFIDF_HW_XGB = XGB_Model(pred,data[\"Education\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stacking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlxtend.classifier import StackingClassifier\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "STKC = StackingClassifier(classifiers=[W2V_nn_XGB, W2V_HW_XGB, TFIDF_nn_XGB, TFIDF_HW_XGB],\n",
    "                          use_probas=False, # 类别概率值作为meta-classfier的输入\n",
    "                          average_probas=False,  # 是否对每一个类别产生的概率值做平均\n",
    "                          meta_classifier=xgb.XGBClassifier())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[17:14:54] WARNING: D:\\Build\\xgboost\\xgboost-1.3.1.git\\src\\learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:15:09] WARNING: D:\\Build\\xgboost\\xgboost-1.3.1.git\\src\\learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:15:26] WARNING: D:\\Build\\xgboost\\xgboost-1.3.1.git\\src\\learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:15:42] WARNING: D:\\Build\\xgboost\\xgboost-1.3.1.git\\src\\learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\Anaconda3_5.2.0\\lib\\site-packages\\xgboost\\sklearn.py:892: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[17:15:58] WARNING: D:\\Build\\xgboost\\xgboost-1.3.1.git\\src\\learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "StackingClassifier(classifiers=[XGBClassifier(base_score=0.5, booster='gbtree',\n",
       "                                              colsample_bylevel=1,\n",
       "                                              colsample_bynode=1,\n",
       "                                              colsample_bytree=1, gamma=0,\n",
       "                                              gpu_id=-1, importance_type='gain',\n",
       "                                              interaction_constraints='',\n",
       "                                              learning_rate=0.300000012,\n",
       "                                              max_delta_step=0, max_depth=6,\n",
       "                                              min_child_weight=1, missing=nan,\n",
       "                                              monotone_constraints='()',\n",
       "                                              n_estimators=100, n_jobs=12,\n",
       "                                              num_par...\n",
       "                                                 importance_type='gain',\n",
       "                                                 interaction_constraints=None,\n",
       "                                                 learning_rate=None,\n",
       "                                                 max_delta_step=None,\n",
       "                                                 max_depth=None,\n",
       "                                                 min_child_weight=None,\n",
       "                                                 missing=nan,\n",
       "                                                 monotone_constraints=None,\n",
       "                                                 n_estimators=100, n_jobs=None,\n",
       "                                                 num_parallel_tree=None,\n",
       "                                                 random_state=None,\n",
       "                                                 reg_alpha=None,\n",
       "                                                 reg_lambda=None,\n",
       "                                                 scale_pos_weight=None,\n",
       "                                                 subsample=None,\n",
       "                                                 tree_method=None,\n",
       "                                                 validate_parameters=None,\n",
       "                                                 verbosity=None))"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "STKC.fit(X_tr,Y_tr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练集准确率: 0.9657, 测试集准确率: 0.4065\n"
     ]
    }
   ],
   "source": [
    "# 模型预测\n",
    "pred_train = STKC.predict(X_tr)\n",
    "pred_test = STKC.predict(X_te)\n",
    "\n",
    "# 输出预测结果\n",
    "train_acc = accuracy_score(Y_tr, pred_train)\n",
    "test_acc = accuracy_score(Y_te, pred_test)\n",
    "print (\"训练集准确率: {0:.4f}, 测试集准确率: {1:.4f}\".format(train_acc, test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

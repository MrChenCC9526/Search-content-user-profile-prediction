{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import jieba\n",
    "import config # 自定义配置文件\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# 不显示VisibleDeprecation警告\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=Warning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 导入数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(config.AF_Data_path + \"AFAfter.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 17651 entries, 0 to 17650\n",
      "Data columns (total 16 columns):\n",
      " #   Column      Non-Null Count  Dtype  \n",
      "---  ------      --------------  -----  \n",
      " 0   Education   17651 non-null  int64  \n",
      " 1   Id          17651 non-null  object \n",
      " 2   age         17651 non-null  int64  \n",
      " 3   gender      17651 non-null  int64  \n",
      " 4   query       17651 non-null  object \n",
      " 5   SpaceNum    17651 non-null  float64\n",
      " 6   SpaceRATIO  17651 non-null  float64\n",
      " 7   LinkNum     17651 non-null  float64\n",
      " 8   LinkRATIO   17651 non-null  float64\n",
      " 9   TextSum     17651 non-null  float64\n",
      " 10  TextMax     17651 non-null  float64\n",
      " 11  TextMin     17651 non-null  float64\n",
      " 12  TextMedian  17651 non-null  float64\n",
      " 13  TextMean    17651 non-null  float64\n",
      " 14  SearchNum   17651 non-null  float64\n",
      " 15  HighWords   17651 non-null  object \n",
      "dtypes: float64(10), int64(3), object(3)\n",
      "memory usage: 2.2+ MB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 导入自定义词库"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建停用词列表\n",
    "def stopwordslist():\n",
    "    # 获取停用词文件列表\n",
    "    for root, dirs, files in os.walk(config.stopword_path):\n",
    "        pass\n",
    "    # 停用词list\n",
    "    stopwords = []\n",
    "    for filename in files:\n",
    "        for line in open(config.stopword_path + filename, 'r+', encoding='utf-8').readlines():\n",
    "            stopword = line.strip()\n",
    "            stopwords.append(stopword)\n",
    "    return list(set(stopwords))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建新增词汇\n",
    "def add_Words():\n",
    "    # 获取自定义词典文件名\n",
    "    for root, dirs, files in os.walk(config.addword_path):\n",
    "        filenames = [filename for filename in files if 'txt' in filename]\n",
    "    addWords = []\n",
    "    for filename in filenames:\n",
    "        for line in open(config.addword_path + filename, 'r+').readlines():\n",
    "            addword = line.strip()\n",
    "            addWords.append(addword)\n",
    "    return list(set(addWords))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "addwordslist = add_Words() # 自定义词典\n",
    "stopwordslist = stopwordslist() # 停用词词典"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache C:\\Users\\11147\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 0.552 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    }
   ],
   "source": [
    "# jieba.add_word(word) # 新增词汇\n",
    "jieba.load_userdict(addwordslist) # 添加用户自定义词典"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 停用词处理\n",
    "def Deactivate_Words(words,stopwordslist):\n",
    "    if len(words) <= 2:\n",
    "        word_After_Stop = words\n",
    "    elif len(words) > 2:\n",
    "        word_After_Stop = list(set(words).difference(stopwordslist)) \n",
    "    return word_After_Stop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 分词"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### jieba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 分词处理\n",
    "def get_tokens(wordtxt):\n",
    "    tokens = [] # 用于存储分词结果\n",
    "    for query in wordtxt.split('\\t'): # 使用 \\t 识别搜内容文本\n",
    "        words = [word for word in jieba.cut(query)] # 分词\n",
    "        words = Deactivate_Words(words,stopwordslist) # 停用词处理\n",
    "#         print(words)\n",
    "        # 分词结果进行拼接；强化词语语义\n",
    "        for gram in [1,2]:\n",
    "            for i in range(len(words) - gram + 1): # 根据分词结果列表进行拼接，最大拼接长度：2\n",
    "                tokens += [\"_*_\".join(words[i:i+gram])] # 使用join拼接词语，并将结果添加至tokens\n",
    "#                 print(\"_*_\".join(words[i:i+gram]))\n",
    "#         print(\"-\"*20)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "PJ_df = pd.DataFrame(columns=[\"token\"])\n",
    "for word in data[\"query\"]:\n",
    "    tokens = get_tokens(word)\n",
    "    df_dict = {\"token\":tokens}\n",
    "    PJ_df = PJ_df.append(df_dict,ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 17651 entries, 0 to 17650\n",
      "Data columns (total 1 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   token   17651 non-null  object\n",
      "dtypes: object(1)\n",
      "memory usage: 138.0+ KB\n"
     ]
    }
   ],
   "source": [
    "PJ_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### FoolNLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "FL_df = pd.DataFrame(columns=[\"token\"])\n",
    "for HighWords in data[\"HighWords\"]:\n",
    "    hw_words = HighWords[2:-2].split(\"', '\")\n",
    "    df_dict = {\"token\":hw_words}\n",
    "    FL_df = FL_df.append(df_dict,ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 17651 entries, 0 to 17650\n",
      "Data columns (total 1 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   token   17651 non-null  object\n",
      "dtypes: object(1)\n",
      "memory usage: 138.0+ KB\n"
     ]
    }
   ],
   "source": [
    "FL_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 分词结果持久化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "PJ_df.to_excel(config.WordData_path + \"Participle-Jieba.xlsx\",encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "FL_df.to_excel(config.WordData_path + \"Participle-FoolNLTK-HW.xlsx\",encoding=\"utf-8\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
